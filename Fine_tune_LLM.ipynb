{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaCV_lArBayK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- 1. Install Libraries ---\n",
        "!pip install \"unsloth[colab-new]\" transformers peft bitsandbytes datasets trl\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import time\n",
        "import os\n",
        "from huggingface_hub import login, snapshot_download\n",
        "\n",
        "# --- 2. HUGGING FACE LOGIN ---\n",
        "login(token=\"HF_TOKEN_FOR_THIS_WORKER_GOES_HERE\")\n",
        "print(\"--- ‚úÖ Hugging Face Login Successful ---\")\n",
        "\n",
        "# --- 3. Load the *BASE* Model ---\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "print(\"--- ‚úÖ Base model loaded ---\")\n",
        "\n",
        "# --- 4. Add PEFT/LoRA Adapters ---\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 42,\n",
        "    max_seq_length = max_seq_length,\n",
        ")\n",
        "\n",
        "# --- 5. Load and Format Your Dataset ---\n",
        "full_dataset = load_dataset(\"ccibeekeoc42/english_to_igbo\", split=\"train\")\n",
        "\n",
        "def format_example(example):\n",
        "    if 'English' in example and 'Igbo' in example and \\\n",
        "       isinstance(example['English'], str) and isinstance(example['Igbo'], str):\n",
        "        text = f\"<s>[INST] Translate this English sentence to Igbo: '{example['English']}' [/INST] {example['Igbo']}</s>\"\n",
        "        return {\"text\": text}\n",
        "    else:\n",
        "        return {\"text\": None}\n",
        "\n",
        "formatted_dataset = full_dataset.map(\n",
        "    format_example,\n",
        "    remove_columns=list(full_dataset.features),\n",
        "    num_proc = 4\n",
        ")\n",
        "\n",
        "formatted_dataset = formatted_dataset.filter(lambda example: example.get(\"text\") is not None)\n",
        "print(f\"--- Dataset loaded. Number of examples: {len(formatted_dataset)} ---\")\n",
        "\n",
        "\n",
        "# --- 6. The Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "    max_steps = 32646,\n",
        "    per_device_train_batch_size = 8,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    optim = \"adamw_torch\",\n",
        "    learning_rate = 2e-5,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    save_strategy = \"steps\",\n",
        "    save_steps = 1000,\n",
        "    save_total_limit = 1,\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = \"nwokikeonyeka/igbo-phi3-checkpoint\",\n",
        "    hub_strategy = \"checkpoint\",\n",
        "    logging_steps = 500,\n",
        "    fp16 = True,\n",
        "    group_by_length = True,\n",
        "    report_to = \"none\",\n",
        ")\n",
        "\n",
        "# --- 7. Initialize the Trainer ---\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    args = training_args,\n",
        "    packing = True,\n",
        ")\n",
        "\n",
        "print(\"--- ‚úÖ All setup is complete! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the \"Colab Relay Race\" (Worker 1 ONLY)\n",
        "\n",
        "print(f\"--- üöÄ STARTING the 'Colab Relay Race' (Worker 1) ---\")\n",
        "print(f\"--- This will run from step 0 and create the first checkpoint. ---\")\n",
        "\n",
        "start_time_train = time.time()\n",
        "\n",
        "try:\n",
        "    # Worker 1 just calls .train() to start from scratch\n",
        "    trainer.train()\n",
        "    print(\"\\n--- üéâ TRAINING COMPLETED NORMALLY! ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- üí• Training interrupted by unexpected error: {e} ---\")\n",
        "\n",
        "finally:\n",
        "    end_time_train = time.time()\n",
        "    print(f\"--- Training run duration: {(end_time_train - start_time_train) / 60:.2f} minutes ---\")\n",
        "    print(\"--- üõë Session ended. The first checkpoint is safe on Hugging Face Hub. ---\")"
      ],
      "metadata": {
        "id": "aFg1q3d9yrqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8RhOXuTK707"
      },
      "outputs": [],
      "source": [
        "# Resume Cell (WORKER 2, 3, ...) after colab session timeout\n",
        "\n",
        "# --- Configuration ---\n",
        "HUB_MODEL_ID = \"nwokikeonyeka/igbo-phi3-checkpoint\"\n",
        "HUB_CHECKPOINT_SUBFOLDER = \"last-checkpoint\"\n",
        "LOCAL_CHECKPOINT_PATH = os.path.join(os.path.expanduser(\"~\"), \"local_hub_resume\")\n",
        "\n",
        "print(f\"--- üëü RESUMING Training (Worker 2/3/...) ---\")\n",
        "print(f\"--- ‚¨áÔ∏è Downloading checkpoint from Hub: {HUB_MODEL_ID}/{HUB_CHECKPOINT_SUBFOLDER} ---\")\n",
        "\n",
        "# --- 1. Download Checkpoint Files Locally ---\n",
        "snapshot_download(\n",
        "    repo_id=HUB_MODEL_ID,\n",
        "    allow_patterns=[f\"{HUB_CHECKPOINT_SUBFOLDER}/*\"],\n",
        "    local_dir=LOCAL_CHECKPOINT_PATH,\n",
        "    local_dir_use_symlinks=False, # Use False for more stability in Colab\n",
        ")\n",
        "\n",
        "# --- 2. Define the Local Path to Resume From ---\n",
        "RESUME_PATH = os.path.join(LOCAL_CHECKPOINT_PATH, HUB_CHECKPOINT_SUBFOLDER)\n",
        "print(f\"--- üéØ Resuming from LOCAL PATH: {RESUME_PATH} ---\")\n",
        "\n",
        "# --- 3. Run the Training ---\n",
        "start_time_train = time.time()\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint = RESUME_PATH)\n",
        "    print(\"\\n--- üéâ TRAINING COMPLETED NORMALLY! ---\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- üí• Training interrupted by unexpected error: {e} ---\")\n",
        "\n",
        "finally:\n",
        "    end_time_train = time.time()\n",
        "    print(f\"--- Training run duration: {(end_time_train - start_time_train) / 60:.2f} minutes ---\")\n",
        "    print(\"--- üõë Session ended. The latest checkpoint is safe on Hugging Face Hub. ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Final GGUF Model\n",
        "\n",
        "# --- 1. Install Libraries ---\n",
        "!pip install \"unsloth[colab-new]\" transformers peft bitsandbytes datasets trl accelerate\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from huggingface_hub import login\n",
        "\n",
        "# --- 2. HUGGING FACE LOGIN ---\n",
        "login(token=\"HF_TOKEN_FOR_THIS_WORKER_GOES_HERE\")\n",
        "print(\"--- ‚úÖ Hugging Face Login Successful ---\")\n",
        "\n",
        "# --- 3. Load The *TRAINED* Model ---\n",
        "# This loads our checkpoint repo, which includes the base model\n",
        "# and the adapter. This is the correct object to use.\n",
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"--- Loading your trained model from 'nwokikeonyeka/igbo-phi3-checkpoint'... ---\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"nwokikeonyeka/igbo-phi3-checkpoint\", # Load the checkpoint\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "print(\"--- ‚úÖ Trained model and adapter loaded successfully! ---\")\n",
        "\n",
        "# --- 4. Push the FINAL Merged Model to the Hub ---\n",
        "# We do NOT call model.merge_and_unload()\n",
        "# The push_to_hub_gguf function handles the merging internally.\n",
        "\n",
        "FINAL_MODEL_NAME = \"igbo-phi3-translator\"\n",
        "\n",
        "print(f\"--- Pushing final GGUF model to 'nwokikeonyeka/{FINAL_MODEL_NAME}'... ---\")\n",
        "print(\"This creates a small, fast file perfect for the Oracle server.\")\n",
        "\n",
        "# Set the tokenizer's template to the Llama-2 [INST] format we trained on.\n",
        "tokenizer.chat_template = (\n",
        "    \"{{ bos_token }}<s>[INST] {{ messages[0]['content'] }} [/INST] \"\n",
        "    \"{{ messages[1]['content'] }}</s>{{ eos_token }}\"\n",
        ")\n",
        "print(\"--- ‚úÖ Llama-2 chat template applied to tokenizer. ---\")\n",
        "# -------------------------\n",
        "\n",
        "model.push_to_hub_gguf(\n",
        "    repo_id = f\"nwokikeonyeka/{FINAL_MODEL_NAME}\",\n",
        "    tokenizer = tokenizer,\n",
        "    quantization_method = \"q4_k_m\"\n",
        ")\n",
        "\n",
        "print(f\"--- ‚úÖ‚úÖ‚úÖ FINAL MODEL SAVED TO {FINAL_MODEL_NAME}! ---\")"
      ],
      "metadata": {
        "id": "ZwZy1kt2FGdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}