{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND/yhY2sN3uffYlf+kZl3U"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 1. Install the SIMPLE, CPU-only version ---\n",
        "print(\"---  installing llama-cpp-python (CPU)... ---\")\n",
        "# This is the simple install. No build tools needed.\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "print(\"\\n--- ‚úÖ All libraries installed! ---\")\n",
        "\n",
        "# --- 2. Import Libraries ---\n",
        "from llama_cpp import Llama\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "# --- 3. Define and Download Your Model ---\n",
        "MODEL_NAME = \"phi-3-mini-4k-instruct.Q4_K_M.gguf\"\n",
        "REPO_ID = \"nwokikeonyeka/igbo-phi3-translator\"\n",
        "\n",
        "print(f\"\\n--- ‚¨áÔ∏è Downloading {MODEL_NAME} from {REPO_ID} ---\")\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=REPO_ID,\n",
        "    filename=MODEL_NAME\n",
        ")\n",
        "print(f\"--- ‚úÖ Model downloaded to: {model_path} ---\")\n",
        "\n",
        "# --- 4. Load the Model onto the CPU ---\n",
        "print(\"--- üß† Loading model onto CPU... (This may take a moment) ---\")\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=0,  # 0 = Use CPU ONLY\n",
        "    n_ctx=1024,      # Context size\n",
        "    verbose=False    # Silence llama.cpp logs\n",
        ")\n",
        "print(\"--- ‚úÖ Model loaded! Ready to test. ---\")\n",
        "\n",
        "# --- 5. Start the Interactive Test Loop ---\n",
        "print(\"\\n--- ü§ñ Igbo Translator Test ---\")\n",
        "print(\"This model is a specialist. It only responds to the 'Translate' prompt.\")\n",
        "print(\"Type 'quit' or 'exit' to stop.\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_prompt = str(input(\"English: \"))\n",
        "    except EOFError:\n",
        "        break\n",
        "\n",
        "    if user_prompt.lower() in [\"quit\", \"exit\"]:\n",
        "        print(\"--- üëã Test ended. ---\")\n",
        "        break\n",
        "    if not user_prompt.strip():\n",
        "        continue\n",
        "\n",
        "    # 1. Format the prompt EXACTLY as it was trained\n",
        "    full_prompt = f\"Translate this English sentence to Igbo: '{user_prompt}'\"\n",
        "    formatted_input = f\"<s>[INST] {full_prompt} [/INST]\"\n",
        "\n",
        "    print(\"Igbo AI: ...thinking...\")\n",
        "\n",
        "    # 2. Run Inference\n",
        "    response = llm(\n",
        "        formatted_input,\n",
        "        max_tokens=100,          # Max length of the translation\n",
        "        stop=[\"</s>\", \"[INST]\"], # Stop when it finishes its response\n",
        "        echo=False               # Don't repeat the prompt\n",
        "    )\n",
        "\n",
        "    # 3. Print the clean result\n",
        "    try:\n",
        "        ai_response = response[\"choices\"][0][\"text\"].strip()\n",
        "        print(f\"Igbo AI: {ai_response}\")\n",
        "    except (IndexError, KeyError):\n",
        "        print(\"Igbo AI: (No response generated. Make sure you typed in English.)\")"
      ],
      "metadata": {
        "id": "jViSIJRGxmzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}